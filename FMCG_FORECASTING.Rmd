---
title: "Predicting the sales of FMCG categorical"
author: "Daniel Lumban Gaol"
date: "6/07/2021"
output: 
  html_document:
    theme: flatly
    higlight: zenburn
    toc: true
    toc_float:
      collapsed: false
    number_sections: true
    df_print: paged
---



```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)

options(scipen = 999)
```

# Case

 A large Indian retail chain has stores across 3 states in India: Maharashtra, Telangana and Kerala. These stores stock products across various categories such as FMCG (fast moving consumer goods), eatables / perishables and others. Managing the inventory is crucial for the revenue stream of the retail chain. Meeting the demand is important to not lose potential revenue, while at the same time stocking excessive products could lead to losses.   
 
```{r, out.width = "100%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("fmcg.jpg")
```
In this case i will building a time series for forcasting the sales of product for one month from fmcg categorical


# Library

```{r}
library(dplyr) # for data wrangling
library(lubridate) # to dea with date
library(padr) # for padding
library(forecast) # time series library
library(tseries) # for adf.test
library(MLmetrics) # calculate error
library(zoo) #imputation missing value
library(tseries) # adf.test
```


# Read Data

This data is obtained from Kaggle, which is product daily sales data from each store in India   
```{r} 
fmcg <- read.csv("data.csv")
glimpse(fmcg)
```

Data Description :   
date : date of sales obsevation   
product_identifier : the id for a product   
department_identifier : The id for spesific department in a store   
category_of_product : The category to which a product belongs   
outlet : The id for a store   
state : The name of the state   
sales : The number of sales product   

# Data Wrangling
```{r}
fmcg <- fmcg %>% 
  mutate(category_of_product = as.factor(category_of_product),
         state = as.factor(state),
         date = ymd(date))
head(fmcg)
```

## Data Agregation

From the data set we just concern to sales forecast in fmcg, so we must to get date and sales from fmcg category product

```{r}
fmcg_data <- fmcg %>% 
             filter(category_of_product == "fast_moving_consumer_goods") %>% 
             select(date,sales)
```

```{r}
daily_fmcg <- fmcg_data %>% 
  group_by(date) %>% 
  summarise(sales = sum(sales)) %>% 
  arrange(date)
daily_fmcg
```


A good time series data is having data that is sequential and not empty, therefore by using a pad, it will check each row of data whether there are blanks or not.

```{r}
daily_fmcg %>% filter(sales == 0)
```

```{r}
daily_fmcg <- daily_fmcg[daily_fmcg$sales > 0, ]
daily_fmcg %>% filter(sales == 0 )
```

change the value of sales 0 to the value of sales on the previous day
```{r}
daily_fmcg %>% 
  arrange(date) %>% 
  pad() %>% 
  na.locf()
```


```{r}
range(daily_fmcg$date)
```
# Time Series object & EDA

```{r}
fmcg_ts <- ts(data = daily_fmcg$sales,
              start = range(daily_fmcg$date)[[1]],
              frequency = 7) #weekly seasonality
```

In this process I will conduct an analysis to find out whether the data are seasonal and trend, one seasonal or more than one seasonal

```{r}
fmcg_decom <- decompose(fmcg_ts)
autoplot(fmcg_decom)
```

From the results of the plot above, it can be said that the data is multiseasonal, and the trend data pattern still shows other seasonal patterns

```{r}
#making 2nd ts object
daily_fmcg$sales %>% 
  msts(seasonal.periods = c(7,7*4)) %>%  # multiseasonal ts (weekly, monthly)
  mstl() %>% #multiseasonal ts decomposotion
  autoplot()
```

```{r}
#making 3nd ts object
daily_fmcg$sales %>% 
  msts(seasonal.periods = c(7*4*3, 7*4*12)) %>%  # multiseasonal ts (quarterly, yearly)
  mstl() %>% #multiseasonal ts decomposotion
  autoplot()
```

the last object is to conduct three time series model experiments, and the last time series model is a model that fits two seasonal and downward trends.

```{r}
#assign final ts object
fmcg_msts <- daily_fmcg$sales %>% 
  msts(seasonal.periods = c(7*4*3, 7*4*12))

# check for stationary
adf.test(fmcg_msts)
```

Based on Augmented Dickey-Fuller Test (adf.test) result, the p-value is < alpha (0.05) and therefore the data is already stationary. Therefore, for a model building using SARIMA, we do not need to perform differencing on the data first.

# Cross Validation

In cross validation I will take test data for the model for one month or four weeks

```{r}
fmcg_train <- function(x){
  train <- head(x, length(x) - 7*4)
}

fmcg_test <- function(x){
  test <- tail(x, 7*4)
}

train_1 <- fmcg_train(fmcg_msts)
test_1 <- fmcg_test(fmcg_msts) 
```


# Model Building

For modeling we will compare ets holt winters with Seasonal Arima, because the data has seasonal and trend    

There is one way to get decomposed data but still maintain information from all the data we have, that is by using **STL (Seasonal Trend with Loess)**. STL will conceptually smooth the neighboring data for each observation by giving a heavier weight to the data that is close to the observed data.  

To model the STL results, we can also apply the exponential smoothing (ETS) and ARIMA methods. In addition, STLM can be used as an alternative way to catch seasonal which cannot be caught by the usual ETS and ARIMA methods.

```{r}
#ets Holt - Winters
fmcg_ets <- stlm(train_1, method = "ets", lambda = 0) # no log transformation for addivite data

# SARIMA
fmcg_arima <- stlm(train_1, method = "arima", lambda = 0)

```


# Forecas & Evaluation

```{r}
compare_forecast <- function(x, test){
  lapply(x, function(x) forecast::accuracy(x, test)["Test set", ]) %>%
    lapply(., function(x) x %>% t() %>% as.data.frame) %>% 
    bind_rows() %>% 
    mutate(model = names(x)) %>%
    select(model, everything())
}
```


```{r}
#Forecast
forecast_ets <- forecast(object = fmcg_ets, h = length(test_1))
forecast_arima <- forecast(object = fmcg_arima, h = length(test_1))
```



```{r}
#Evaluation
forecast_list <- list(
  "ETS" = forecast_ets,
  "Arima" = forecast_arima
)
compare_forecast(forecast_list,test_1 )
```

From the comparison results above, Arima produces a smaller MAPE than ETS   

```{r}
fmcg_msts %>% 
  autoplot(series = "Actual") +
  autolayer(forecast_ets$mean, series = "ETS") + 
  autolayer(forecast_arima$mean, series = "ARIMA")
```

# Shapiro Test

```{r}
shapiro.test(forecast_arima$residuals)
```

```{r}
hist(forecast_arima$residuals, breaks = 50)
```


```{r}
TSstudio::test_forecast(actual = fmcg_msts, 
              forecast.obj = forecast_arima, 
              train = train_1, 
              test = test_1)
```



# Predicting

Using model has selected   
```{r}
model_fmcg <- fmcg_msts
```

> Data predict

Data test for one month,and we will only choose categorical fmcg and accumulate daily from each store
```{r}
predict <- read.csv("test_data.csv")
```

```{r}
predict <- predict %>% 
  filter(category_of_product == "fast_moving_consumer_goods") %>% 
  select(-id)
```


```{r}
predict_clean <- predict %>% mutate(date = ymd(date))
```

```{r}
predict_forecast <- predict_clean[!duplicated(predict_clean$date),] %>% 
  select(date)
predict_forecast
```

# Model

forecast using the Arima model model   

```{r}
arima_model <- stlm(model_fmcg, method = "arima", lambda = 0)

#forecast
arima_f <- forecast(arima_model, h = 31)# the day in predict
```

```{r}
model_fmcg %>% 
  autoplot(series = "Actual") +
  autolayer(arima_f$mean, series = "Forecast")
```
# Data frame predict

```{r}
predict_forecast %>% 
  cbind(arima_f) %>% as.data.frame()
```


# Conclutions
From the results of the analysis above, the data has a multiseasonal pattern, namely quarterly and yearly, and the Arima model is the best model with a 20% MAPE so that the accuracy of the model can be said to be 80% for forecasting.







